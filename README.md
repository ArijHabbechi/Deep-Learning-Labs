# Deep-Learning-Labs
This repository contains the lab assignments for a Deep Learning course. Each lab covers a different aspect of neural networks, ranging from simple perceptrons to more complex architectures like Convolutional and Recurrent Neural Networks.

## Labs Overview

1. **Perceptron Mono-Couche (Single-Layer Perceptron)**
   - This lab focuses on implementing a basic single-layer perceptron, one of the simplest forms of neural networks. The model is trained and evaluated on simple datasets.
   - **Topics Covered:**
     - Perceptron algorithm
     - Activation functions
     - Basic gradient descent

2. **DNN (Deep Neural Networks)**
   - In this lab, we dive into more complex deep neural network architectures with multiple hidden layers. We explore how increasing the depth of the network impacts learning and performance.
   - **Topics Covered:**
     - Deep learning theory
     - Backpropagation
     - Optimizers like Adam and SGD

3. **Les Réseaux de Neurones Convolutifs (Convolutional Neural Networks)**
   - This lab is focused on Convolutional Neural Networks (CNNs), which are particularly effective for image data. We implement CNNs for tasks such as image classification.
   - **Topics Covered:**
     - Convolutional layers
     - Pooling layers
     - Data augmentation for image tasks

4. **Les Réseaux de Neurones Récurrents (Recurrent Neural Networks)**
   - This lab explores Recurrent Neural Networks (RNNs) and their applications in sequence data such as time series or natural language processing. We implement basic RNNs for processing sequential data.
   - **Topics Covered:**
     - Recurrent layers
     - Long Short-Term Memory (LSTM)
     - Sequence prediction
     
Each lab is provided as a Jupyter Notebook (`.ipynb`) file.
